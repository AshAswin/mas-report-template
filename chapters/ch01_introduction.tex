%!TEX root = ../report.tex

\begin{document}
    \chapter{Introduction}
    \section{Motivation}
	Deep Neural Network models are increasingly becoming a part of everyday software systems. This is due to their ability to exhibit state-of-the-art-performance in terms of accuracy, which often surpasses that of classical methods. Their increased adoption can also be attributed to availability of high-performance computing hardware and software tools that enable easier expression and training of models. \enquote{As the barriers to building machine learning systems become lower, there is rising excitement around the idea of applying the technology in high-impact domains} \cite{schulam2019can}like medical diagnosis, autonomous driving and finance. However, the tendency of Neural Network models to output over-confident predictions\cite{lakshminarayanan2017simple} poses a limitation to their use in such areas. For instance, an overconfident incorrect prediction from a Neural Network model responsible for pedestrian detection in an autonomous car, can lead to an accident that costs a few human lives. Therefore, it is important to quantify the uncertainty linked  to predictions of a Neural Network model, which could be in turn used by its host system for better decision-making.
	\\
	Apart from increasing reliability of Neural Networks, estimating uncertainty can be helpful to improve the accuracy of Neural Network models. For example, performance of a Neural Network classification model that produces highly uncertain output for inputs belonging to a particular class can be improved by retraining the Network with more samples of the deficient class.\cite{settles2012active}\cite{yang2016active}
	\\
	The Deep Learning research community has devised methods (for example \cite{amini2020deep}, \cite{gal2016dropout}, \cite{lakshminarayanan2017simple}) to estimate uncertainty in Neural Network predictions, that enable models to output predictive distributions in place of point estimates and quantify uncertainty in terms of scale/variance of those distributions. The methods are integrated to a Neural Network model by making some changes in its architecture, optimization process and/or inference procedure. Each method varies based on the nature of its target Neural Network's task and also by its approach to estimate uncertainty.
	\\
	A significant portion of research conducted on the topic of uncertainty estimation has been attributed to methods for classification networks (\cite{sensoy2018evidential}, \cite{malinin2018predictive}) than regression. Even among literature on methods for regression nets, many focus on proving their technique's success over others, by comparing with a few selected methods on a limited number of aspects. Also, there does not exist a comprehensive benchmark for such a class of uncertainty estimation methods. This research work satisfies the need for such a benchmark which would help the Deep Learning (DL) research community identify potential methods for further research and aid practitioners in choosing a suitable method based on their requirements.     
    \section{Challenges and Difficulties}
    The following is a list of issues that need to be addressed by uncertainty estimation methods and a benchmark meant for them:
    \begin{itemize}
    	\item An uncertainty estimation method is often integrated as a supplement to a deterministic Neural Net model to enable it output distributions in place of point estimates. Such an integration can have following consequences on the host model:
    	\begin{itemize}
		\item Increased inference/test time
		\item Increased model size
		\item Major architectural changes that affect model's predictive accuracy
		\item Changes in optimization process
    	\end{itemize}
    	\item An uncertainty estimation method with many control parameters is not favored, as it adds to the burden of DL practitioners to tweak them based on the problem at hand.
    	\item It is important for an uncertainty estimation method to rely more on data to model uncertainties than on assumptions about distributions underlying them.
    	\item An alignment between estimated uncertainty levels and prediction error is expected. In other words, \enquote{a network should provide a calibrated confidence measure}\cite{guo2017on}.
    	\item There does not exist a ground truth or a target for the value of uncertainty linked to a prediction. Therefore, it is important to devise a suitable strategy for evaluating uncertainty estimation methods.
    	\item An uncertainty estimation method is commonly expected to produce relatively higher valued uncertainty estimates for Out-Of-Distribution(OOD) inputs than ones that fall within the training data distribution. However, this does not hold true practically for all cases. Invariance of a neural network model to changes in certain features that differentiate in and out of distribution inputs has a role to play. This has to be taken into consideration, while evaluating uncertainty estimation methods on OOD inputs.
        \end{itemize}

    \section{Problem Statement}
    This research work aims to identify, evaluate and compare state-of-the-art uncertainty estimation methods for regression nets. Though uncertainty estimation methods for NN models are broadly classified into Bayesian and non-Bayesian approaches, there exist other aspects that need to be considered before choosing a method for a diverse and reasonable benchmark. Such additional aspects considered to choose methods for this research work are: number of forward passes required, need for architectural changes, need for changes in optimization, need for an ensemble and choice of distribution to model prior over weights. The chosen set of methods are carefully analyzed and two out of them are picked for extensive evaluation.\\
    As mentioned earlier, one of the predominant applications of uncertainty estimation methods is selective prediction in safety-critical systems. Therefore, it is prudent to evaluate the chosen pair of methods on a dataset related to a safety-critical application. In this research work, the Udacity steering angle dataset is used to evaluate the selected pair of methods. Apart from the steering angle dataset, a group of 1D datasets are also used benchmark the methods of choice for following benefits:
    \begin{itemize}
    	
    	\item Ease in visualizing results
    	\item Improved control over parameters (related to both Neural Network training and uncertainty estimation methods)
    	\item Creates possibilities to compare with baseline uncertainty estimation methods such as Gaussian Processes
    	\item Reduced experimentation time
    \end{itemize}
	On both steering angle and 1D datasets, the chosen pair of uncertainty estimation methods are evaluated based on uncertainty estimation quality and prediction accuracy. However, when it comes to measuring quality of uncertainty estimates, commonly used metrics such as (NLL) Negative-Log-Likelihood are often misleading\cite{yao2019quality}. As this work also uses NLL, the metric is carefully analyzed and its limitations are reported.
	
	\enquote{A key use of uncertainty estimation is to understand when a model is faced with test samples that fall out-of-distribution (OOD) or when the model's output cannot be trusted}\cite{amini2020deep}.
	The last sections(\ref{subsec_ood},\ref{subsec_adv}) of this work, evaluate the chosen pair of methods on appropriateness of their response to Out-Of-Distribution inputs and adversarial attacks, which \enquote{pose a serious threat to the success of deep learning in practice}\cite{akhtar2018threat}. Such an evaluation is also crucial to assess the usefulness of an uncertainty estimation method in reporting high levels of uncertainty to the host system, when its corresponding NN model encounters a less familiar or an unknown input.
\end{document}
