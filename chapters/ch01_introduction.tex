%!TEX root = ../report.tex

\begin{document}
    \chapter{Introduction}
    \section{Motivation}
	Deep Neural Network models are increasingly becoming a part of everyday software systems. This is due to their ability to exhibit state-of-the-art-performance in terms of accuracy, which often surpasses that of classical methods. Their increased adoption can also be attributed to the availability of high-performance computing hardware and software tools that enable easier expression and training of models. \enquote{As the barriers to building machine learning systems become lower, there is rising excitement around the idea of applying the technology in high-impact domains} \cite{schulam2019can} like medical diagnosis, autonomous driving and finance. However, the tendency of neural network models to output over-confident predictions \cite{lakshminarayanan2017simple} poses a limitation to their use in such areas. For instance, an overconfident incorrect prediction from a neural network model responsible for pedestrian detection in an autonomous car, can lead to an accident that costs human lives. Therefore, it is important to quantify the uncertainty linked  to predictions of a neural network model, which could be in turn used by its host system to subject predictions with low-confidence to further validation or reject them (selective prediction). Apart from increasing the reliability of neural networks, estimating uncertainty can also be helpful in improving their model accuracies. For example, the performance of a neural network classification model that produces highly uncertain output for inputs belonging to a particular class, can be improved by retraining the network with more samples of the deficient class. \cite{settles2012active} \cite{yang2016active}


	The Deep Learning research community has devised methods (for example \cite{amini2020deep}, \cite{gal2016dropout}, \cite{lakshminarayanan2017simple}) to estimate uncertainty in neural network predictions, that enable models to output predictive distributions in place of point estimates and quantify uncertainty in terms of scale/variance of those distributions. An uncertainty estimation method is integrated to a neural network model by making some changes in its architecture, optimization process and/or inference procedure. Each method varies based on the nature of its target neural network's task and also by its approach to estimating uncertainty.


	A significant portion of research conducted on the topic of uncertainty estimation has been attributed to methods for classification networks ( \cite{sensoy2018evidential},  \cite{malinin2018predictive}) in contrast to regression. Even among literature on methods for regression nets, many focus on proving their technique's success over others, by comparing with a few selected methods on a limited number of aspects. Also, there does not exist a comprehensive benchmark for such a class of uncertainty estimation methods. This research work satisfies the need for such a benchmark which would help the Deep Learning (DL) research community identify potential methods for further research and aid practitioners in choosing a suitable method based on their requirements.     
    \section{Challenges and Difficulties}
    The following is a list of issues that need to be addressed by any uncertainty estimation method:
    \begin{itemize}
    	\item An uncertainty estimation method is often integrated as a supplement to a deterministic neural net model to enable it output distributions in place of point estimates. Such an integration can have following consequences on the model:
    	\begin{itemize}
		\item Increased inference/test time
		\item Increased model size
		\item Major architectural changes that affect model's prediction accuracy
		\item Changes in optimization process
    	\end{itemize}
    	\item An uncertainty estimation method with many control parameters is not favored, as it adds to the burden of DL practitioners to tweak them based on the problem at hand.
    	\item It is important for an uncertainty estimation method to rely more on data to model uncertainties than on assumptions about their underlying distributions.
    	\item An alignment between estimated uncertainty levels and prediction error is expected. In other words, \enquote{a network should provide a calibrated confidence measure} \cite{guo2017on}.
    	\item There does not exist a ground truth or a target for the value of uncertainty linked to a prediction. Therefore, it is important to devise a suitable strategy for evaluating uncertainty estimation methods.
    	\item An uncertainty estimation method is commonly expected to produce relatively higher valued uncertainty estimates for Out-Of-Distribution (OOD) inputs than ones that fall within the training data distribution. However, this does not hold true practically for all cases. Invariance of a neural network model to changes in certain features that differentiate in and out of distribution inputs has a role to play. This has to be taken into consideration, while evaluating uncertainty estimation methods on OOD inputs.
        \end{itemize}

    \section{Problem Statement}
    This research work aims to identify, evaluate and compare state-of-the-art uncertainty estimation methods for regression nets. Though uncertainty estimation methods for neural network models are broadly classified into Bayesian and non-Bayesian approaches, there exist other aspects that need to be considered before choosing a method for a diverse and a reasonable benchmark. The additional aspects are: number of forward passes required, need for architectural changes, need for changes in optimization, need for an ensemble and choice of distribution to model prior over weights. The chosen set of methods are carefully analyzed and two out of them are picked for an intensive evaluation.
    
    
    As mentioned earlier, one of the predominant applications of uncertainty estimation methods is selective prediction in safety-critical systems. Therefore, it is prudent to evaluate the chosen pair of methods on a dataset related to a safety-critical application. In this research work, the Udacity steering angle dataset \cite{udasteer} is used for evaluating the methods. Apart from the steering angle dataset, a group of 1D datasets are also used to benchmark the methods of choice for the following benefits:
    \begin{itemize}
    	
    	\item Ease in visualizing results
    	\item Improved control over parameters (related to both neural network training and uncertainty estimation methods)
    	\item Creates possibilities to compare with baseline uncertainty estimation methods such as Gaussian Processes (GP)
    	\item Reduced experimentation time
    \end{itemize}
	On both steering angle and 1D datasets, the chosen pair of uncertainty estimation methods are evaluated based on uncertainty estimation quality and prediction accuracy. However, when it comes to measuring the quality of uncertainty estimates, commonly used metrics such as Negative-Log-Likelihood (NLL) are often misleading \cite{yao2019quality}. As this work also uses NLL, the metric is carefully analyzed and its limitations are reported.
	
	\enquote{A key use of uncertainty estimation is to understand when a model is faced with test samples that fall out-of-distribution (OOD) or when the model's output cannot be trusted} \cite{amini2020deep}.
	The last sections (Section \ref{subsec_ood}, Section \ref{subsec_adv}) of this work, evaluate the chosen pair of methods on appropriateness of their response to OOD inputs and adversarial attacks, which \enquote{pose a serious threat to the success of deep learning in practice} \cite{akhtar2018threat}. Such an evaluation is also crucial to assess the usefulness of an uncertainty estimation method in reporting high levels of uncertainty to the host system, when its corresponding NN model encounters a less familiar or an unknown input.
	To put everything concisely, this research work addresses the following research questions:
	\begin{itemize}
		\item \textbf{RQ1. }What are the existing state-of-the-art uncertainty estimation
		methods in Deep Learning for regression tasks?
		\item \textbf{RQ2. }How do the identified uncertainty estimation methods compare
		with each other and GP models, based on their uncertainty estimation quality and prediction accuracy of their respective models?
		\item \textbf{RQ3. }How do the identified uncertainty estimation methods compare with each other based on their response to OOD and adversarially perturbed inputs?
	\end{itemize}
\end{document}
